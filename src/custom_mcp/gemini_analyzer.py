"""
Gemini APIã‚’ä½¿ç”¨ã—ãŸã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æã¨ãƒãƒ£ãƒ—ã‚¿ãƒ¼ç”Ÿæˆ
YouTubeå‹•ç”»ã®ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æ„å‘³ã®ã‚ã‚‹ç›®æ¬¡ã«å¤‰æ›
"""

import json
import os
import re
from datetime import timedelta
from typing import Any

import google.generativeai as genai
from youtube_transcript_api import YouTubeTranscriptApi


class GeminiTranscriptAnalyzer:
    """Gemini APIã‚’ä½¿ç”¨ã—ãŸãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆåˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self, api_key: str | None = None):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: Gemini API key (ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ã‹ã‚‰ã‚‚å–å¾—å¯èƒ½)
        """
        if api_key:
            self.api_key = api_key
        else:
            self.api_key = os.getenv('GEMINI_API_KEY')

        if not self.api_key:
            raise ValueError("Gemini API key ãŒå¿…è¦ã§ã™ã€‚ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ã‚’è¨­å®šã™ã‚‹ã‹ã€api_key ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

        # Gemini APIè¨­å®š
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel('gemini-1.5-pro')

    def format_timestamp(self, seconds: float) -> str:
        """ç§’ã‚’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼ï¼ˆHH:MM:SSï¼‰ã«å¤‰æ›"""
        td = timedelta(seconds=int(seconds))
        hours, remainder = divmod(td.seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    def get_transcript(self, video_id: str) -> list[dict[str, Any]]:
        """YouTubeå‹•ç”»ã®ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å–å¾—"""
        try:
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ja', 'en'])
            return transcript
        except Exception as e:
            raise Exception(f"ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    def create_segments_for_analysis(self, transcript: list[dict], segment_duration: int = 300) -> list[dict]:
        """
        åˆ†æç”¨ã«ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²
        
        Args:
            transcript: YouTubeãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
            segment_duration: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é•·ã•ï¼ˆç§’ï¼‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5åˆ†
        """
        segments = []
        current_segment = {
            "start_time": 0,
            "end_time": 0,
            "text": "",
            "transcript_entries": []
        }

        for entry in transcript:
            start_time = entry['start']
            text = entry['text']

            # æ–°ã—ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é–‹å§‹åˆ¤å®š
            if start_time - current_segment["start_time"] >= segment_duration:
                if current_segment["text"]:
                    current_segment["end_time"] = start_time
                    segments.append(current_segment.copy())

                # æ–°ã—ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–‹å§‹
                current_segment = {
                    "start_time": start_time,
                    "end_time": start_time,
                    "text": text,
                    "transcript_entries": [entry]
                }
            else:
                current_segment["text"] += " " + text
                current_segment["transcript_entries"].append(entry)
                current_segment["end_time"] = start_time + entry.get('duration', 0)

        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
        if current_segment["text"]:
            segments.append(current_segment)

        return segments

    def analyze_segment_with_gemini(
        self, 
        segment: dict[str, Any],
        granularity: str = "medium",
        custom_prompt: str | None = None
    ) -> dict[str, Any]:
        """
        Geminiã‚’ä½¿ç”¨ã—ã¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’åˆ†æ
        
        Args:
            segment: åˆ†æå¯¾è±¡ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
            
        Returns:
            åˆ†æçµæœï¼ˆãƒˆãƒ”ãƒƒã‚¯ã€æ¦‚è¦ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç­‰ï¼‰
        """

        # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if custom_prompt:
            prompt = f"""
ä»¥ä¸‹ã®YouTubeå‹•ç”»ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š

ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ:
ã€Œ{segment['text']}ã€

æ™‚é–“: {self.format_timestamp(segment['start_time'])} - {self.format_timestamp(segment['end_time'])}

{custom_prompt}

å›ç­”ã¯å¿…ãšJSONå½¢å¼ã§ä»¥ä¸‹ã®é …ç›®ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
- topic_category: å†…å®¹ã‚«ãƒ†ã‚´ãƒªãƒ¼
- title: ã‚¿ã‚¤ãƒˆãƒ«
- summary: è¦ç´„
- key_points: é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆé…åˆ—ï¼‰
- technical_level: æŠ€è¡“ãƒ¬ãƒ™ãƒ«
- contains_demo: ãƒ‡ãƒ¢ã®æœ‰ç„¡
- contains_code: ã‚³ãƒ¼ãƒ‰ã®æœ‰ç„¡
"""
        else:
            # ç²’åº¦ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´
            title_length = {
                "fine": 20,
                "medium": 15,
                "coarse": 10
            }.get(granularity, 15)
            
            summary_length = {
                "fine": 100,
                "medium": 50,
                "coarse": 30
            }.get(granularity, 50)
            
            key_points_count = {
                "fine": 5,
                "medium": 3,
                "coarse": 2
            }.get(granularity, 3)
            
            granularity_instruction = {
                "fine": "éå¸¸ã«è©³ç´°ã«åˆ†æã—ã€ç´°ã‹ãªãƒˆãƒ”ãƒƒã‚¯ã®å¤‰åŒ–ã‚‚æ‰ãˆã¦ãã ã•ã„ã€‚",
                "medium": "ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨æ¦‚å¿µã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚",
                "coarse": "æœ€ã‚‚é‡è¦ãªä¸»é¡Œã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"
            }.get(granularity, "")

            prompt = f"""
ä»¥ä¸‹ã¯ YouTube å‹•ç”»ã®ä¸€éƒ¨åˆ†ã®ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚{granularity_instruction}
ä»¥ä¸‹ã®æƒ…å ±ã‚’JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ï¼š

ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ:
ã€Œ{segment['text']}ã€

æ™‚é–“: {self.format_timestamp(segment['start_time'])} - {self.format_timestamp(segment['end_time'])}

åˆ†æã—ã¦ã»ã—ã„é …ç›®:
1. topic_category: ã“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆä¾‹: "å°å…¥", "æŠ€è¡“è§£èª¬", "ãƒ‡ãƒ¢", "è³ªç–‘å¿œç­”", "ã¾ã¨ã‚" ãªã©ï¼‰
2. title: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ç°¡æ½”ãªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ{title_length}æ–‡å­—ä»¥å†…ã€YouTubeç›®æ¬¡ã«é©ã—ãŸå½¢ï¼‰
3. summary: å†…å®¹ã®è¦ç´„ï¼ˆ{summary_length}æ–‡å­—ä»¥å†…ï¼‰
4. key_points: é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼ˆé…åˆ—ã€æœ€å¤§{key_points_count}å€‹ï¼‰
5. technical_level: æŠ€è¡“ãƒ¬ãƒ™ãƒ«ï¼ˆ"beginner", "intermediate", "advanced", "general"ï¼‰
6. contains_demo: ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼ˆtrue/falseï¼‰
7. contains_code: ã‚³ãƒ¼ãƒ‰ã®èª¬æ˜ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼ˆtrue/falseï¼‰

å›ç­”ã¯å¿…ãšJSONå½¢å¼ã®ã¿ã§è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚

ä¾‹:
{{
  "topic_category": "æŠ€è¡“è§£èª¬",
  "title": "AWS LambdaåŸºç¤",
  "summary": "ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹é–¢æ•°ã®åŸºæœ¬æ¦‚å¿µã¨ä½¿ç”¨æ–¹æ³•ã‚’èª¬æ˜",
  "key_points": ["ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹", "Lambdaé–¢æ•°", "å®Ÿè¡Œç’°å¢ƒ"],
  "technical_level": "beginner",
  "contains_demo": false,
  "contains_code": true
}}
"""

        try:
            response = self.model.generate_content(prompt)

            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            response_text = response.text.strip()

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONéƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆå¿µã®ãŸã‚ï¼‰
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                analysis = json.loads(json_str)
            else:
                analysis = json.loads(response_text)

            return analysis

        except Exception as e:
            print(f"Geminiåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ
            return {
                "topic_category": "ãã®ä»–",
                "title": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³",
                "summary": segment['text'][:50] + "..." if len(segment['text']) > 50 else segment['text'],
                "key_points": [],
                "technical_level": "general",
                "contains_demo": False,
                "contains_code": False
            }

    def generate_semantic_chapters(
        self, 
        video_id: str, 
        segment_duration: int = 300,
        granularity: str = "medium",
        custom_prompt: str | None = None
    ) -> dict[str, Any]:
        """
        ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æã«ã‚ˆã‚‹æ„å‘³ã®ã‚ã‚‹ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            video_id: YouTubeå‹•ç”»ID
            segment_duration: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·ï¼ˆç§’ï¼‰
            granularity: ç²’åº¦ãƒ¬ãƒ™ãƒ« ("fine", "medium", "coarse", "custom")
                - fine: è©³ç´°ãªåˆ†å‰²ï¼ˆ2-3åˆ†ã”ã¨ï¼‰
                - medium: æ¨™æº–çš„ãªåˆ†å‰²ï¼ˆ5åˆ†ã”ã¨ï¼‰
                - coarse: å¤§ã¾ã‹ãªåˆ†å‰²ï¼ˆ10åˆ†ä»¥ä¸Šï¼‰
                - custom: ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨
            custom_prompt: ã‚«ã‚¹ã‚¿ãƒ åˆ†ææŒ‡ç¤ºï¼ˆgranularity="custom"æ™‚ã«ä½¿ç”¨ï¼‰
            
        Returns:
            ãƒãƒ£ãƒ—ã‚¿ãƒ¼æƒ…å ±ã‚’å«ã‚€è¾æ›¸
        """
        # ç²’åº¦ã«å¿œã˜ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·ã®èª¿æ•´
        if granularity == "fine":
            segment_duration = min(segment_duration, 180)  # 3åˆ†ä»¥ä¸‹
        elif granularity == "coarse":
            segment_duration = max(segment_duration, 600)  # 10åˆ†ä»¥ä¸Š
        elif granularity == "custom" and not custom_prompt:
            raise ValueError("custom_promptãŒå¿…è¦ã§ã™ï¼ˆgranularity='custom'ã®å ´åˆï¼‰")
        print(f"ğŸ¬ å‹•ç”» {video_id} ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æé–‹å§‹...")

        # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå–å¾—
        transcript = self.get_transcript(video_id)
        print(f"âœ… {len(transcript)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå–å¾—å®Œäº†")

        # åˆ†æç”¨ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²
        segments = self.create_segments_for_analysis(transcript, segment_duration)
        print(f"ğŸ“Š {len(segments)}å€‹ã®åˆ†æã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²")

        # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’Geminiã§åˆ†æ
        analyzed_chapters = []
        for i, segment in enumerate(segments):
            print(f"ğŸ” ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {i+1}/{len(segments)} åˆ†æä¸­...")

            analysis = self.analyze_segment_with_gemini(segment, granularity, custom_prompt)

            chapter = {
                "index": i + 1,
                "start_seconds": segment["start_time"],
                "end_seconds": segment["end_time"],
                "start_timestamp": self.format_timestamp(segment["start_time"]),
                "end_timestamp": self.format_timestamp(segment["end_time"]),
                "duration_seconds": segment["end_time"] - segment["start_time"],
                "original_text": segment["text"],
                **analysis
            }

            analyzed_chapters.append(chapter)
            print(f"   âœ… {chapter['title']} ({chapter['topic_category']})")

        # å‹•ç”»ã®ç·æ™‚é–“
        total_duration = transcript[-1]['start'] + transcript[-1]['duration']

        result = {
            "video_id": video_id,
            "total_duration_seconds": total_duration,
            "total_duration_formatted": self.format_timestamp(total_duration),
            "total_chapters": len(analyzed_chapters),
            "analysis_method": "gemini_semantic",
            "segment_duration": segment_duration,
            "granularity": granularity,
            "custom_prompt_used": custom_prompt is not None,
            "chapters": analyzed_chapters
        }

        return result

    def format_for_youtube_description(self, chapters_data: dict[str, Any]) -> str:
        """
        YouTubeèª¬æ˜æ¬„ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸç›®æ¬¡ã‚’ç”Ÿæˆ
        
        Args:
            chapters_data: generate_semantic_chapters()ã®æˆ»ã‚Šå€¤
            
        Returns:
            YouTubeèª¬æ˜æ¬„ã«ãã®ã¾ã¾è²¼ã‚Šä»˜ã‘å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆ
        """

        lines = []
        lines.append("ğŸ“‹ ç›®æ¬¡ / Table of Contents")
        lines.append("=" * 40)
        lines.append("")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        category_groups = {}
        for chapter in chapters_data['chapters']:
            category = chapter['topic_category']
            if category not in category_groups:
                category_groups[category] = []
            category_groups[category].append(chapter)

        # ã‚«ãƒ†ã‚´ãƒªé †åº
        category_order = ["å°å…¥", "æŠ€è¡“è§£èª¬", "ãƒ‡ãƒ¢", "å®Ÿè£…", "è³ªç–‘å¿œç­”", "ã¾ã¨ã‚", "ãã®ä»–"]

        for category in category_order:
            if category in category_groups:
                lines.append(f"## {category}")
                for chapter in category_groups[category]:
                    line = f"{chapter['start_timestamp']} {chapter['title']}"
                    if chapter['summary']:
                        line += f" - {chapter['summary']}"
                    lines.append(line)
                lines.append("")

        # ãã®ä»–ã®ã‚«ãƒ†ã‚´ãƒª
        for category, chapters in category_groups.items():
            if category not in category_order:
                lines.append(f"## {category}")
                for chapter in chapters:
                    line = f"{chapter['start_timestamp']} {chapter['title']}"
                    if chapter['summary']:
                        line += f" - {chapter['summary']}"
                    lines.append(line)
                lines.append("")

        # æŠ€è¡“ãƒ¬ãƒ™ãƒ«è¡¨ç¤º
        tech_levels = set(ch['technical_level'] for ch in chapters_data['chapters'])
        if tech_levels:
            lines.append("ğŸ¯ æŠ€è¡“ãƒ¬ãƒ™ãƒ«:")
            level_names = {
                "beginner": "åˆå¿ƒè€…å‘ã‘",
                "intermediate": "ä¸­ç´šè€…å‘ã‘",
                "advanced": "ä¸Šç´šè€…å‘ã‘",
                "general": "ä¸€èˆ¬å‘ã‘"
            }
            for level in tech_levels:
                lines.append(f"  â€¢ {level_names.get(level, level)}")
            lines.append("")

        # è‡ªå‹•ç”Ÿæˆã®æ³¨è¨˜
        lines.append("---")
        lines.append("ğŸ¤– ã“ã®ç›®æ¬¡ã¯AIã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
        lines.append(f"ğŸ“º å‹•ç”»æ™‚é–“: {chapters_data['total_duration_formatted']}")
        lines.append(f"ğŸ“Š ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ•°: {chapters_data['total_chapters']}")

        return "\n".join(lines)

    def save_results(self, video_id: str, chapters_data: dict[str, Any]):
        """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""

        # 1. è©³ç´°åˆ†æçµæœã‚’JSONä¿å­˜
        with open(f"semantic_chapters_{video_id}.json", 'w', encoding='utf-8') as f:
            json.dump(chapters_data, f, ensure_ascii=False, indent=2)

        # 2. YouTubeèª¬æ˜æ¬„ç”¨ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜
        youtube_description = self.format_for_youtube_description(chapters_data)
        with open(f"youtube_description_{video_id}.txt", 'w', encoding='utf-8') as f:
            f.write(youtube_description)

        # 3. ç°¡æ˜“ãƒãƒ£ãƒ—ã‚¿ãƒ¼ãƒªã‚¹ãƒˆä¿å­˜
        with open(f"simple_chapters_{video_id}.txt", 'w', encoding='utf-8') as f:
            for chapter in chapters_data['chapters']:
                f.write(f"{chapter['start_timestamp']} {chapter['title']}\n")

        print("\nğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:")
        print(f"  - semantic_chapters_{video_id}.json (è©³ç´°åˆ†æ)")
        print(f"  - youtube_description_{video_id}.txt (YouTubeèª¬æ˜æ¬„ç”¨)")
        print(f"  - simple_chapters_{video_id}.txt (ç°¡æ˜“ãƒªã‚¹ãƒˆ)")


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    if not os.getenv('GEMINI_API_KEY'):
        print("âŒ GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("Google AI Studio (https://aistudio.google.com/) ã§APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ãã ã•ã„")
        return

    video_id = "MlIxNOTYMcc"

    try:
        analyzer = GeminiTranscriptAnalyzer()

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æå®Ÿè¡Œ
        chapters_data = analyzer.generate_semantic_chapters(video_id, segment_duration=300)

        # çµæœä¿å­˜
        analyzer.save_results(video_id, chapters_data)

        # çµæœè¡¨ç¤º
        print("\nğŸ‰ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æå®Œäº†!")
        print(f"å‹•ç”»: {video_id}")
        print(f"æ™‚é–“: {chapters_data['total_duration_formatted']}")
        print(f"ãƒãƒ£ãƒ—ã‚¿ãƒ¼: {chapters_data['total_chapters']}å€‹")

        print("\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸãƒãƒ£ãƒ—ã‚¿ãƒ¼:")
        for chapter in chapters_data['chapters']:
            print(f"  {chapter['start_timestamp']} {chapter['title']} ({chapter['topic_category']})")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()
