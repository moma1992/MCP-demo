#!/usr/bin/env python3
"""
ç²’åº¦èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
from dotenv import load_dotenv
from src.custom_mcp.gemini_analyzer import GeminiTranscriptAnalyzer

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

def test_different_granularities():
    """ç•°ãªã‚‹ç²’åº¦ã§ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æãƒ†ã‚¹ãƒˆ"""
    
    if not os.getenv('GEMINI_API_KEY'):
        print("âŒ GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False
    
    video_id = "MlIxNOTYMcc"
    analyzer = GeminiTranscriptAnalyzer()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
    test_cases = [
        {
            "name": "è©³ç´°åˆ†æ (fine)",
            "granularity": "fine",
            "segment_duration": 180,  # 3åˆ†
            "custom_prompt": None
        },
        {
            "name": "æ¨™æº–åˆ†æ (medium)",
            "granularity": "medium", 
            "segment_duration": 300,  # 5åˆ†
            "custom_prompt": None
        },
        {
            "name": "å¤§ã¾ã‹åˆ†æ (coarse)",
            "granularity": "coarse",
            "segment_duration": 600,  # 10åˆ†
            "custom_prompt": None
        },
        {
            "name": "ã‚«ã‚¹ã‚¿ãƒ åˆ†æ (æ•™è‚²çš„è¦³ç‚¹)",
            "granularity": "custom",
            "segment_duration": 300,
            "custom_prompt": """
ã“ã®å‹•ç”»ã‚’æ•™è‚²çš„ãªè¦³ç‚¹ã‹ã‚‰åˆ†æã—ã¦ãã ã•ã„ï¼š
- å­¦ç¿’ç›®æ¨™ã¨ãªã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’æ˜ç¢ºã«
- åˆå­¦è€…ãŒç†è§£ã—ã‚„ã™ã„é †åºã§æ•´ç†
- å®Ÿè·µçš„ãªå†…å®¹ã¨ãƒ‡ãƒ¢ã‚’åŒºåˆ¥
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é›£æ˜“åº¦ã‚’æ˜ç¤º
ã‚¿ã‚¤ãƒˆãƒ«ã¯å­¦ç¿’ã®è¦³ç‚¹ã‹ã‚‰åˆ†ã‹ã‚Šã‚„ã™ãå‘½åã—ã¦ãã ã•ã„ã€‚
"""
        },
        {
            "name": "ã‚«ã‚¹ã‚¿ãƒ åˆ†æ (ãƒ“ã‚¸ãƒã‚¹å‘ã‘)",
            "granularity": "custom",
            "segment_duration": 300,
            "custom_prompt": """
ã“ã®å‹•ç”»ã‚’ãƒ“ã‚¸ãƒã‚¹è¦³ç‚¹ã‹ã‚‰åˆ†æã—ã¦ãã ã•ã„ï¼š
- ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã‚„ROIã«é–¢é€£ã™ã‚‹å†…å®¹ã‚’é‡è¦–
- æŠ€è¡“çš„è©³ç´°ã‚ˆã‚Šæ¦‚å¿µçš„ç†è§£ã‚’å„ªå…ˆ
- æ„æ€æ±ºå®šã«å½¹ç«‹ã¤æƒ…å ±ã‚’æŠ½å‡º
- ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã«é©ã—ãŸå½¢å¼ã§
"""
        }
    ]
    
    print(f"ğŸ¬ å‹•ç”» {video_id} ã®ç²’åº¦åˆ¥ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ” ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i}/{len(test_cases)}: {test_case['name']}")
        print("-" * 60)
        
        try:
            # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æå®Ÿè¡Œ
            chapters_data = analyzer.generate_semantic_chapters(
                video_id,
                test_case['segment_duration'],
                test_case['granularity'],
                test_case['custom_prompt']
            )
            
            # çµæœè¡¨ç¤º
            print(f"âœ… åˆ†æå®Œäº†")
            print(f"  - ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ•°: {chapters_data['total_chapters']}")
            print(f"  - ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·: {chapters_data['segment_duration']}ç§’")
            print(f"  - ç²’åº¦: {chapters_data['granularity']}")
            
            # æœ€åˆã®3ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã‚’è¡¨ç¤º
            print(f"\n  ç”Ÿæˆã•ã‚ŒãŸãƒãƒ£ãƒ—ã‚¿ãƒ¼ï¼ˆæœ€åˆã®3å€‹ï¼‰:")
            for j, chapter in enumerate(chapters_data['chapters'][:3], 1):
                print(f"  {j}. {chapter['start_timestamp']} {chapter['title']}")
                if test_case['granularity'] == "custom":
                    print(f"     â†’ {chapter['summary']}")
            
            if len(chapters_data['chapters']) > 3:
                print(f"  ... ä»– {len(chapters_data['chapters']) - 3} ãƒãƒ£ãƒ—ã‚¿ãƒ¼")
            
            # çµæœã‚’ä¿å­˜
            filename = f"semantic_{video_id}_{test_case['granularity']}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                import json
                json.dump(chapters_data, f, ensure_ascii=False, indent=2)
            print(f"\n  ğŸ’¾ çµæœã‚’ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"\nğŸ¯ ç²’åº¦æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Œäº†")
    print("=" * 80)
    print("\nğŸ“Š ç²’åº¦ã«ã‚ˆã‚‹é•ã„:")
    print("  - fine: ã‚ˆã‚Šå¤šãã®è©³ç´°ãªãƒãƒ£ãƒ—ã‚¿ãƒ¼ã€çŸ­ã„åŒºé–“")
    print("  - medium: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸåˆ†å‰²")
    print("  - coarse: ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ã®ã¿ã€é•·ã„åŒºé–“")
    print("  - custom: ç‰¹å®šã®è¦³ç‚¹ã‹ã‚‰ã®åˆ†æ")


def compare_chapter_counts():
    """ç”Ÿæˆã•ã‚ŒãŸãƒãƒ£ãƒ—ã‚¿ãƒ¼æ•°ã®æ¯”è¼ƒ"""
    import json
    import glob
    
    print("\nğŸ“ˆ ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ•°ã®æ¯”è¼ƒ:")
    print("-" * 40)
    
    files = glob.glob("semantic_MlIxNOTYMcc_*.json")
    for file in sorted(files):
        try:
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            granularity = data.get('granularity', 'unknown')
            chapters = data.get('total_chapters', 0)
            print(f"{granularity:10} : {chapters:2} ãƒãƒ£ãƒ—ã‚¿ãƒ¼")
        except:
            pass


if __name__ == "__main__":
    test_different_granularities()
    compare_chapter_counts()